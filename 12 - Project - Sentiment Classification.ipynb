{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "12 - Project - Sentiment Classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adel-nouar/ML_with_Rune/blob/main/12%20-%20Project%20-%20Sentiment%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "229fa4f2"
      },
      "source": [
        "# Project: Sentiment Classification\n",
        "- Make a model to determine whether a tweet positive or negative"
      ],
      "id": "229fa4f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd592d25"
      },
      "source": [
        "### Step 1: Import the libraries"
      ],
      "id": "dd592d25"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b246b825"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import classify\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import NaiveBayesClassifier\n",
        "from random import shuffle"
      ],
      "id": "b246b825",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6e9000"
      },
      "source": [
        "### Step 2: Download the sample tweets\n",
        "- Execute the following cell"
      ],
      "id": "1c6e9000"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7c52abb",
        "outputId": "6fa48a3a-cbc5-4d89-a54d-7252191aedf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "id": "d7c52abb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feccae3b"
      },
      "source": [
        "### Step 3: The tweets\n",
        "- Get the positive and negative tweets.\n",
        "    - HINT: You access the positive tweets by: **nltk.corpus.twitter_samples.strings('positive_tweets.json')**\n",
        "    - HINT: Similarly for the negative tweets.\n",
        "- Notice: There is also tweets with no sentiment - we will ignore them in this project\n",
        "- Check a few tweets"
      ],
      "id": "feccae3b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddfb7faa"
      },
      "source": [
        "positive_tweets = nltk.corpus.twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = nltk.corpus.twitter_samples.strings('negative_tweets.json')"
      ],
      "id": "ddfb7faa",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a35fd39b",
        "outputId": "d9fd8827-9143-4ab4-a06c-ed7a02aa4631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "positive_tweets[0]"
      ],
      "id": "a35fd39b",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fac2b506"
      },
      "source": [
        "### Step 4: Tokenize the tweets\n",
        "- You get the tokenized tweets as follows:\n",
        "    - **nltk.corpus.twitter_samples.tokenized('positive_tweets.json')**\n",
        "    - Simlarly for **negative_tweets**\n",
        "- Why tokenize?\n",
        "    - To make processing easier\n",
        "- Check a few tweets (tokenized)"
      ],
      "id": "fac2b506"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfe1210"
      },
      "source": [
        "positive_tweets = nltk.corpus.twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweets = nltk.corpus.twitter_samples.tokenized('negative_tweets.json')"
      ],
      "id": "fbfe1210",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2c04ca3",
        "outputId": "eae82839-81ee-49d6-aaf5-ee333e65c55d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "positive_tweets[0]"
      ],
      "id": "f2c04ca3",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#FollowFriday',\n",
              " '@France_Inte',\n",
              " '@PKuchly57',\n",
              " '@Milipol_Paris',\n",
              " 'for',\n",
              " 'being',\n",
              " 'top',\n",
              " 'engaged',\n",
              " 'members',\n",
              " 'in',\n",
              " 'my',\n",
              " 'community',\n",
              " 'this',\n",
              " 'week',\n",
              " ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce77587"
      },
      "source": [
        "### Step 5: Remove noise from data\n",
        "- The following tokens do not add value in our analysis\n",
        "    - Twitter usernames (starting with @)\n",
        "    - Hyperlinks (starting with http:// or https://)\n",
        "    - Punctuation and special characters\n",
        "        - HINT: if word in **string.punctuation**\n",
        "    - Numeric values only\n",
        "        - HINT: use **.isnumeric()**\n",
        "    - If word is a stopword ([wiki](https://en.wikipedia.org/wiki/Stop_word))\n",
        "        - HINT: Check if lower case word is in **stopwords.words('english')**\n",
        "- To simplify createa a helper function **is_clean** to check for the above\n",
        "- Create another helper function **clean_tokens**\n",
        "    - The function takes **tokens** (a list of tokens) as input\n",
        "    - Then returns a list of tokens, where **is_clean** has been used to filter\n",
        "    - Also, let's lowercase it all\n",
        "        - HINT: Use **lower()**\n",
        "- Finally, use list comprehension on the lists of positive and negative tweets where **clean_tokens** is applied on each element (tokens)."
      ],
      "id": "cce77587"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9443f6a8"
      },
      "source": [
        "def is_clean(word: str):\n",
        "  if word.startswith('@'):\n",
        "    return False\n",
        "  if word.startswith('http://') or word.startswith('https://'):\n",
        "    return False\n",
        "  if word in string.punctuation:\n",
        "    return False\n",
        "  if word.isnumeric():\n",
        "    return False\n",
        "  if word in stopwords.words('english'):\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def clean_tokens(tokens: list):\n",
        "  return [word.lower() for word in tokens if is_clean(word)]\n",
        "\n",
        "positive_tweets_cleaned = [clean_tokens(tokens) for tokens in positive_tweets]\n",
        "negative_tweets_cleaned = [clean_tokens(tokens) for tokens in negative_tweets]"
      ],
      "id": "9443f6a8",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6723188f",
        "outputId": "df60759e-2a6c-424c-b3a2-21ebe3461372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "positive_tweets_cleaned[0]"
      ],
      "id": "6723188f",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#followfriday', 'top', 'engaged', 'members', 'community', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f944678",
        "outputId": "fb5bd27d-9a81-452b-8ceb-f4ee3d93aef0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "negative_tweets_cleaned[0]"
      ],
      "id": "1f944678",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless', 'tmr', ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84a5141e"
      },
      "source": [
        "### Step 6: Normalize the data\n",
        "- The process of converting a word to its canonical form.\n",
        "- Without normalization, “ran”, “runs”, and “running” would be treated as different words.\n",
        "- Create a lemmatizer of **WordNetLemmatizer()**\n",
        "    - HINT: use **lemmatizer = WordNetLemmatizer()**\n",
        "- Create a helper function to lemmatize\n",
        "    - HINT: Create a helper function **lemmatize(word, tag)**\n",
        "        - Convert tag to **n** or **v** if tag starts with **NN** or **VB**, else **a**\n",
        "        - Return **lemmatizer.lemmatize(word, tag)**\n",
        "- Create a helper function **lemmatize_tokens(tokens: list)**\n",
        "    - Return a list, where each element of **word, tag in pos_tag(...)** of **lemmatize(word, tag)**.\n",
        "- Use list comprehension to normalize the positive and negative tweets\n",
        "    - HINT: apply **lemmatize_tokens(...)** on all elements"
      ],
      "id": "84a5141e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e60d6802"
      },
      "source": [
        " lemmatizer = WordNetLemmatizer()\n",
        "\n",
        " def  lemmatize(word: str, tag: str):\n",
        "     if tag.startswith('NN'):\n",
        "       pos = 'n'\n",
        "     elif tag.startswith('VB'):\n",
        "        pos = 'v'\n",
        "     else:\n",
        "        pos = 'a'\n",
        "     return lemmatizer.lemmatize(word, pos)\n",
        "\n",
        "def lemmatize_tokens(tokens:list):\n",
        "  return [lemmatize(word, tag) for word, tag in pos_tag(tokens)]\n",
        "\n",
        "\n",
        "positive_tweets_normalized = [lemmatize_tokens(tokens) for tokens in positive_tweets_cleaned]\n",
        "negative_tweets_normalized = [lemmatize_tokens(tokens) for tokens in negative_tweets_cleaned]"
      ],
      "id": "e60d6802",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5749d93",
        "outputId": "702362de-7ad7-4354-9eac-76110598faa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "positive_tweets_normalized[0]"
      ],
      "id": "c5749d93",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#followfriday', 'top', 'engaged', 'member', 'community', 'week', ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d63b870",
        "outputId": "a19ec98c-d78b-44e2-a28a-224600a2c8ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "negative_tweets_normalized[0]"
      ],
      "id": "6d63b870",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hopeless', 'tmr', ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ef7fc28"
      },
      "source": [
        "### Step 7: Prepare data for Model\n",
        "- Example of normalized tweet: **['hopeless', 'tmr', ':(']**\n",
        "    - Should become **({'hopeless': True, 'tmr': True, ':(': True}, 'Negative')**\n",
        "- Hence, the list of tweets (positive and negative) should be converted\n",
        "- HINT: use a dict comprehension inside a list comprehension"
      ],
      "id": "4ef7fc28"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78005ef"
      },
      "source": [
        "positive_dataset = [({token: True for token in tokens}, 'Positive') for tokens in positive_tweets_normalized]\n",
        "negative_dataset = [({token: True for token in tokens}, 'Negative') for tokens in negative_tweets_normalized]"
      ],
      "id": "a78005ef",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8237eeeb",
        "outputId": "200aca9c-3866-4fc8-edda-0d01dc460433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "positive_dataset[0]"
      ],
      "id": "8237eeeb",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'#followfriday': True,\n",
              "  ':)': True,\n",
              "  'community': True,\n",
              "  'engaged': True,\n",
              "  'member': True,\n",
              "  'top': True,\n",
              "  'week': True},\n",
              " 'Positive')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5353eb86",
        "outputId": "ce985ec3-c09b-434e-8b57-0ca9c397dccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "negative_dataset[0]"
      ],
      "id": "5353eb86",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({':(': True, 'hopeless': True, 'tmr': True}, 'Negative')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdb20c6b"
      },
      "source": [
        "### Step 8: Prepare training and test dataset\n",
        "- Make the dataset of the combined positive and negative datasets\n",
        "- Shuffle the dataset\n",
        "    - Use **shuffle**\n",
        "- Let the training dataset be the first 7000 entries\n",
        "- Let the test dataset be the remaining entries"
      ],
      "id": "fdb20c6b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2e53a1"
      },
      "source": [
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "shuffle(dataset)\n",
        "\n",
        "train_ds = dataset[:7000]\n",
        "test_ds = dataset[7000:]"
      ],
      "id": "3d2e53a1",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a776bf28"
      },
      "source": [
        "### Step 9: Train and test Model\n",
        "- Train the model:\n",
        "    - HINT: **classifier = NaiveBayesClassifier.train(train_data)**\n",
        "- Test the accuracy\n",
        "    - HINT: **classify.accuracy(classifier, test_data)**"
      ],
      "id": "a776bf28"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa55202d"
      },
      "source": [
        "classifier = NaiveBayesClassifier.train(train_ds)"
      ],
      "id": "fa55202d",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95761195",
        "outputId": "377deb71-d6bd-47ca-ea28-0462cee7dfae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classify.accuracy(classifier, test_ds)"
      ],
      "id": "95761195",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.993"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b47f6d"
      },
      "source": [
        "### Step 10: Show the most informative features\n",
        "- HINT: Get the 10 most informative features: **classifier.show_most_informative_features(10)**"
      ],
      "id": "d8b47f6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bdd74e6",
        "outputId": "75a8c31e-d83b-4f98-8d5d-b68ac5efdfdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "id": "6bdd74e6",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2044.2 : 1.0\n",
            "                      :) = True           Positi : Negati =   1661.0 : 1.0\n",
            "                 welcome = True           Positi : Negati =     35.0 : 1.0\n",
            "                follower = True           Positi : Negati =     23.5 : 1.0\n",
            "                     sad = True           Negati : Positi =     23.1 : 1.0\n",
            "                     bam = True           Positi : Negati =     20.1 : 1.0\n",
            "                     too = True           Negati : Positi =     19.0 : 1.0\n",
            "                     ugh = True           Negati : Positi =     13.4 : 1.0\n",
            "                    blog = True           Positi : Negati =     13.3 : 1.0\n",
            "                    sick = True           Negati : Positi =     13.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9087a7d5"
      },
      "source": [
        "### Step 11: Test the model\n",
        "- Try your model as follows:\n",
        "    - Define a tweet: **tweet = 'this is fun and awesome'**\n",
        "    - Prepare data for model: **tweet_dict = {token: True for token in lemmatize_tokens(clean_tokens(tweet.split()))}**\n",
        "    - Classify data: **classifier.classify(tweet_dict)**"
      ],
      "id": "9087a7d5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0f5be5b"
      },
      "source": [
        "tweet = 'this is fun and awesome'\n",
        "tweet_dict = {token: True for token in lemmatize_tokens(clean_tokens(tweet.split()))}"
      ],
      "id": "d0f5be5b",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5ba7919",
        "outputId": "4f523197-b47a-4635-d994-9734fe5e08a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "classifier.classify(tweet_dict)"
      ],
      "id": "e5ba7919",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46b1dd34"
      },
      "source": [
        "### Bonus: The pre-trained Sentiment Intensity Analyzer\n",
        "-  VADER (Valence Aware Dictionary and sEntiment Reasoner) ([Vader](https://www.nltk.org/howto/sentiment.html))"
      ],
      "id": "46b1dd34"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7848af9",
        "outputId": "b49c05cc-35d5-4451-bef0-f64091ecdd78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ],
      "id": "a7848af9",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2687d9f"
      },
      "source": [
        "sia = SentimentIntensityAnalyzer()"
      ],
      "id": "d2687d9f",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e73154c",
        "outputId": "1715b36f-5030-488d-e0ed-3a8b47d38d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sia.polarity_scores('this is fun and awesome')"
      ],
      "id": "0e73154c",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.8126, 'neg': 0.0, 'neu': 0.288, 'pos': 0.712}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}