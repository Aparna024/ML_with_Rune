{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning\n",
    "### Goal of lesson\n",
    "- Understand how Reinforcement Learning works\n",
    "- Learn about Agent and Environment\n",
    "- How it iterates and gets rewards based on action\n",
    "- How to continuesly learn new things\n",
    "- Create own Reinforcement Learning from scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reinforcement Learning simply explained\n",
    "- Given a set of rewards or punishments, learn what actions to take in the future\n",
    "- The second large group of Machine Learning\n",
    "\n",
    "### Environment\n",
    "<img src='img/reinforcement-learning.png' width=600 align='left'>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Agent\n",
    "- The environment gives the agent a state\n",
    "- The agent action\n",
    "- The environment gives a state and reward (or punishment)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is how robots are taught how to walk"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Markov Decision Process\n",
    "- Model for decision-making, representing states, actions, and their rewards\n",
    "- Set of states $S$\n",
    "- Set of actions $Actions(s)$\n",
    "- Transition model $P(s'|s, a)$\n",
    "- Reward function $R(s, a, s')$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-learning (one model)\n",
    "- Method for learning a function $Q(s, a)$, estimate of the value of performing action $a$ in state $s$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-learning\n",
    "- Start with $Q(s, a) = 0$ for all $s, a$\n",
    "- Update $Q$ when we take an action\n",
    "- $Q(s, a) = Q(s, a) + \\alpha($reward$ + \\gamma\\max(s', a') - Q(s, a)) = (1 - \\alpha)Q(s, a) + \\alpha($reward$ + \\gamma\\max(s', a'))$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $\\epsilon$-Greedy Decision Making\n",
    "**Explore vs Exploit**\n",
    "- With propability $\\epsilon$ take a random move\n",
    "- Otherwise, take action $a$ with maximum $Q(s, a)$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple task\n",
    "<img src='img/field.png' width=600 align='left'>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Starts at a random point\n",
    "- Move left or right\n",
    "- Avoid the red box\n",
    "- Find the green box"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Field](img/field-3.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Programming Notes:\n",
    "> - Libraries used\n",
    ">     - [**numpy**](http://numpy.org) - scientific computing with Python ([Lecture on NumPy](https://youtu.be/BpzpU8_j0-c))\n",
    ">     - [**random**](https://docs.python.org/3/library/random.html) - pseudo-random generators\n",
    "> - Functionality and concepts used\n",
    ">     - **Object-Oriented Programming (OOP)**: [Lecture on Object Oriented Programming](https://youtu.be/hbO9xo6RfDM)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Resources\n",
    "> #### What if there are more states?\n",
    "> - [Reinforcement Learning from Scratch](https://youtu.be/y4LEVVE2mV8)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class Field:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize field and set a random start state\n",
    "        \"\"\"\n",
    "        self.states = [-1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "        self.state = random.randrange(0, len(self.states))\n",
    "    \n",
    "    def done(self):\n",
    "        \"\"\"\n",
    "        Check if it isn't in a neutral state\n",
    "        \"\"\"\n",
    "        if self.states[self.state] != 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        \"\"\"\n",
    "        Return possible actions in state\n",
    "\n",
    "        Action:\n",
    "               0 => left\n",
    "               1 => right\n",
    "        \"\"\"    \n",
    "        actions = [0, 1]\n",
    "        if self.state == 0:\n",
    "            actions.remove(0)\n",
    "        if self.state == len(self.states) -1:\n",
    "            actions.remove(1)\n",
    "        return actions\n",
    "\n",
    "    def update_next_state(self, action):\n",
    "        \"\"\" \n",
    "        Update state according to action -> Return state and reward\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            if self.state == 0:\n",
    "                return self.state, -10\n",
    "            self.state = -1\n",
    "        if action == 1:\n",
    "            if self.state == len(self.states) -1:\n",
    "                return self.state, -10\n",
    "            self.state += 1\n",
    "        \n",
    "        reward = self.states[self.state]\n",
    "        return self.state, reward"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "field = Field()\n",
    "field.state, field.done(), field.get_possible_actions()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11, False, [0])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "field.update_next_state(1)\n",
    "field.state, field.done(), field.get_possible_actions()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(11, False, [0])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "field.update_next_state(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "field = Field()\n",
    "q_table = np.zeros((len(field.states), 2))\n",
    "\n",
    "alpha = .5\n",
    "epsilon = .5\n",
    "gamma = .5\n",
    "\n",
    "for _ in range(10000):\n",
    "    field = Field()\n",
    "    while not field.done():\n",
    "        actions = field.get_possible_actions()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = np.argmax(q_table[field.state])\n",
    "        \n",
    "        cur_state = field.state\n",
    "        next_state, reward = field.update_next_state(action)\n",
    "        q_table[cur_state, action] = (1 - alpha) * q_table[cur_state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "q_table"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.      ,  0.      ],\n",
       "       [ 0.      ,  0.015625],\n",
       "       [ 0.      ,  0.03125 ],\n",
       "       [ 0.      ,  0.0625  ],\n",
       "       [ 0.      ,  0.125   ],\n",
       "       [ 0.      ,  0.25    ],\n",
       "       [ 0.      ,  0.5     ],\n",
       "       [ 0.      ,  1.      ],\n",
       "       [ 0.      ,  0.      ],\n",
       "       [ 0.      ,  0.      ],\n",
       "       [ 0.      ,  0.      ],\n",
       "       [ 0.      , -1.      ]])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('ML_DS': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "746800f96ece29ab8b5df47a779cb8f76655f7cf39bc7615a93fb17b55f50443"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}